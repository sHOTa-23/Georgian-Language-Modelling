{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a03020-e2a7-48a9-8421-81ee9900ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import Namespace\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "args = Namespace()\n",
    "args.train = \"medium.txt\"\n",
    "args.max_len = 128\n",
    "args.epochs = 1\n",
    "args.batch_size = 4\n",
    "args.token_path = 'georgian-vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77e9a00a-bf76-4c31-bbd3-4c9884ff10bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./georgian-vocab.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=args.train, vocab_size=30_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=[\n",
    "                    '[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(\".\", \"georgian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56672d29-f81e-4eb8-920a-c27d2ec6becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok = BertTokenizer(\n",
    "    args.token_path\n",
    ")\n",
    "# and trai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d8e7b84-1289-46c6-98d1-5bc3343dc629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 25401, 289, 2306, 9937, 13489, 155, 12375, 927, 11255, 1393, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok('გამარჯობა როგორ ხარ მებადური ზღვაშიშევარდნილი')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "953d1f13-0fb2-4f83-a3fa-76219b9c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lines = self.load_lines(file)\n",
    "        self.ids = self.encode_lines(self.lines)\n",
    "        \n",
    "    def load_lines(self, file):\n",
    "        with open(file) as f:\n",
    "            lines = [\n",
    "                line\n",
    "                for line in f.read().splitlines()\n",
    "                if (len(line) > 0 and not line.isspace())\n",
    "            ]\n",
    "        return lines\n",
    "    \n",
    "    def encode_lines(self, lines):\n",
    "        batch_encoding = self.tokenizer(\n",
    "            lines, add_special_tokens=True, truncation=True, max_length=args.max_len\n",
    "        )\n",
    "        return batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        \n",
    "train_dataset = MaskedLMDataset(args.train, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c45f062-8ea4-4ad4-bdbc-6cea5361657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33a5520b-9d91-428e-b50f-4bd627c91146",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=30_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c78e6080-b65c-4cf0-ba79-f12743704d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "bert = BertForMaskedLM(config).to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cafead54-ac36-474c-ac56-1f8b34eabc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66587184"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in bert.parameters())\n",
    "model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ee3dd5b-a540-45b9-a6e5-4d468af68571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=5000,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9253d8f3-40bd-4cfc-b807-85d996de1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6c94d-c02d-4843-952f-04482fe243da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 200000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 62500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9393' max='62500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9393/62500 31:31 < 2:58:16, 4.96 it/s, Epoch 3.01/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>8.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>8.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>8.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>7.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>7.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>7.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>7.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>7.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>7.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>7.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>6.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>6.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>6.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.625200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/checkpoint-5000\n",
      "Configuration saved in model/checkpoint-5000/config.json\n",
      "Model weights saved in model/checkpoint-5000/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "pytorch-gpu.1-11.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
